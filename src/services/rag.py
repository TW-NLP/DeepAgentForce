import os
import json
import shutil
import logging
import hashlib
import uuid
import asyncio
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Optional, Any
import numpy as np
from fastapi import FastAPI, UploadFile, File, HTTPException, BackgroundTasks, APIRouter, Body
from pydantic import BaseModel
import pdfplumber
from docx import Document as DocxDocument
import pandas as pd
import tiktoken
from openai import AsyncOpenAI
from pymilvus import MilvusClient, DataType

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger("MilvusRAG")

# ==================== Pydantic Models (æŽ¥å£å®šä¹‰) ====================

class UploadResponse(BaseModel):
    success: bool
    message: str
    document_id: str
    document_name: str
    chunks_count: int
    uploaded_at: str

class DocumentInfo(BaseModel):
    document_id: str
    name: str
    path: str
    chunks: int
    uploaded_at: str
    metadata: Dict

class ListDocumentsResponse(BaseModel):
    success: bool
    total: int
    documents: List[DocumentInfo]

class IndexStatusResponse(BaseModel):
    success: bool
    is_indexed: bool
    total_documents: int
    total_chunks: int
    message: str

class QueryRequest(BaseModel):
    question: str
    top_k: int = 5 # å¯¹åº”åŽŸæŽ¥å£çš„ top_k_communitiesï¼ŒåŸºç¡€RAGé‡Œå³ä¸º top_k chunks

class QueryResponse(BaseModel):
    success: bool
    question: str
    answer: str
    processing_time: float

class DeleteResponse(BaseModel):
    success: bool
    message: str
    document_id: str

# ==================== åŸºç¡€å·¥å…·ç±» ====================

class DocumentParser:
    """æ–‡æ¡£è§£æžå™¨"""
    @staticmethod
    def parse_file(file_path: Path) -> str:
        ext = file_path.suffix.lower()
        try:
            if ext == '.pdf':
                with pdfplumber.open(file_path) as pdf:
                    return "\n".join([p.extract_text() or "" for p in pdf.pages])
            elif ext in ['.docx', '.doc']:
                doc = DocxDocument(file_path)
                return "\n".join([p.text for p in doc.paragraphs if p.text.strip()])
            elif ext == '.txt':
                with open(file_path, 'r', encoding='utf-8') as f:
                    return f.read()
            elif ext == '.csv':
                df = pd.read_csv(file_path)
                return df.to_string(index=False)
            elif ext in ['.md', '.markdown']:
                with open(file_path, 'r', encoding='utf-8') as f:
                    return f.read()
            return ""
        except Exception as e:
            logger.error(f"è§£æžå¤±è´¥ {file_path}: {e}")
            raise

class TextChunker:
    """åˆ†å—å™¨"""
    def __init__(self, chunk_size=600, overlap=100):
        self.chunk_size = chunk_size
        self.overlap = overlap
        self.encoding = tiktoken.get_encoding("cl100k_base")

    def chunk_text(self, text: str) -> List[str]:
        tokens = self.encoding.encode(text)
        chunks = []
        for i in range(0, len(tokens), self.chunk_size - self.overlap):
            chunk_tokens = tokens[i : i + self.chunk_size]
            chunks.append(self.encoding.decode(chunk_tokens))
        return chunks


class MilvusRAGPipeline():
    """
    åŸºç¡€ RAG Pipeline
    - æ–‡æ¡£å…ƒæ•°æ® -> æœ¬åœ° JSON
    - æ–‡æœ¬å— & å‘é‡ -> Milvus
    """
    
    def __init__(self,settings):
        """
        åˆå§‹åŒ– Milvus å®¢æˆ·ç«¯å’Œ OpenAI å®¢æˆ·ç«¯"""
        self.settings=settings

        self.client_rag = AsyncOpenAI(api_key=self.settings.EMBEDDING_API_KEY, base_url=self.settings.EMBEDDING_URL)

        self.client_llm = AsyncOpenAI(api_key=self.settings.LLM_API_KEY, base_url=self.settings.LLM_URL)
        
        # Milvus Client
        self.milvus = MilvusClient(uri=self.settings.MILVUS_URL)
        self._init_collection()
        
        # æœ¬åœ°å…ƒæ•°æ®å­˜å‚¨ (ç”¨äºŽå¿«é€Ÿåˆ—è¡¨)
        self.meta_file = self.settings.MILVUS_DIR / "doc_metadata.json"
        self.documents = self._load_local_metadata() # {doc_uuid: dict}


    def _init_collection(self):
        """åˆå§‹åŒ– Milvus é›†åˆ"""
        if not self.milvus.has_collection(self.settings.MILVUS_COLLECTION):
            # 1. åˆ›å»ºé›†åˆ (ç®€æ˜“æ¨¡å¼ä¼šè‡ªåŠ¨åˆ›å»ºåä¸º "vector" çš„å‘é‡å­—æ®µ)
            self.milvus.create_collection(
                collection_name=self.settings.MILVUS_COLLECTION,
                dimension=self.settings.EMBEDDING_DIM,
                metric_type="COSINE",
                auto_id=True, 
                enable_dynamic_field=True
            )
            
            # 2. å‡†å¤‡ç´¢å¼•å‚æ•°
            index_params = self.milvus.prepare_index_params()

            # 3. å¿…é¡»å…ˆä¸ºå‘é‡å­—æ®µ "vector" åˆ›å»ºç´¢å¼• (è¿™æ˜¯ç®€æ˜“æ¨¡å¼çš„é»˜è®¤å­—æ®µå)
            index_params.add_index(
                field_name="vector",
                index_type="AUTOINDEX", # è‡ªåŠ¨é€‰æ‹©æœ€åˆé€‚çš„ç´¢å¼•
                metric_type="COSINE"
            )

            # 4. ä¸º doc_id åˆ›å»ºæ ‡é‡ç´¢å¼• (åŠ é€Ÿåˆ é™¤å’Œè¿‡æ»¤)
            # æ³¨æ„ï¼šMilvus ä¸­å­—ç¬¦ä¸²é€šå¸¸ä½¿ç”¨ "STL_SORT" æˆ–é»˜è®¤ç´¢å¼•ç±»åž‹ï¼Œ"Trie" åœ¨æŸäº›ç‰ˆæœ¬æœ‰ç‰¹å®šé™åˆ¶
            # index_params.add_index(
            #     field_name="doc_id",
            #     index_type="INVERTED"  # æŽ¨èä½¿ç”¨å€’æŽ’ç´¢å¼•ï¼ŒåŠ é€Ÿç²¾ç¡®åŒ¹é…
            # )

            # 5. æ‰§è¡Œåˆ›å»ºç´¢å¼•
            self.milvus.create_index(
                collection_name=self.settings.MILVUS_COLLECTION,
                index_params=index_params
            )
            
            logger.info(f"âœ… åˆ›å»º Milvus é›†åˆåŠç´¢å¼•: {self.settings.MILVUS_COLLECTION}")
    def _load_local_metadata(self) -> Dict:
        if self.meta_file.exists():
            with open(self.meta_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        return {}

    def _save_local_metadata(self):
        with open(self.meta_file, 'w', encoding='utf-8') as f:
            json.dump(self.documents, f, ensure_ascii=False, indent=2)

    async def _get_embedding(self, texts: List[str]) -> List[List[float]]:
        """æ‰¹é‡èŽ·å– Embedding"""
        if not texts: return []

        resp = await self.client_rag.embeddings.create(input=texts, model=self.settings.EMBEDDING_MODEL)
        return [d.embedding for d in resp.data]

    # --- åŠŸèƒ½å®žçŽ° ---

    async def add_document(self, file_path: str, metadata: Dict) -> str:
        """è§£æž -> åˆ†å— -> å‘é‡åŒ– -> å­˜å…¥ Milvus"""
        doc_uuid = str(uuid.uuid4())
        
        # 1. è§£æžä¸Žåˆ†å—
        parser = DocumentParser()
        chunker = TextChunker()
        
        text = parser.parse_file(Path(file_path))
        chunks = chunker.chunk_text(text)
        
        if not chunks:
            raise ValueError("æ–‡æ¡£è§£æžä¸ºç©º")

        logger.info(f"ðŸ“„ å¤„ç†æ–‡æ¡£: {metadata['title']} ({len(chunks)} chunks)")

        # 2. å‘é‡åŒ–
        embeddings = await self._get_embedding(chunks)

        # 3. æž„é€  Milvus æ•°æ®
        milvus_data = []
        for chunk_text, vector in zip(chunks, embeddings):
            milvus_data.append({
                "vector": vector,
                "text": chunk_text,
                "doc_id": doc_uuid,
                "source": metadata['original_filename']
            })

        # 4. å†™å…¥ Milvus
        self.milvus.insert(collection_name=self.settings.MILVUS_COLLECTION, data=milvus_data)

        # 5. æ›´æ–°æœ¬åœ°å…ƒæ•°æ®
        metadata.update({
            "uuid": doc_uuid,
            "path": file_path,
            "chunks_count": len(chunks),
            "added_at": datetime.now().isoformat()
        })
        self.documents[doc_uuid] = metadata
        self._save_local_metadata()

        return doc_uuid

    def remove_document(self, doc_id: str):
        """ä»Ž Milvus å’Œæœ¬åœ°å…ƒæ•°æ®ä¸­åˆ é™¤"""
        if doc_id not in self.documents:
            raise ValueError(f"æ–‡æ¡£ ID ä¸å­˜åœ¨: {doc_id}")

        # 1. ä»Ž Milvus åˆ é™¤ (æ ¹æ® doc_id è¿‡æ»¤)
        delete_expr = f'doc_id == "{doc_id}"'
        self.milvus.delete(collection_name=self.settings.MILVUS_COLLECTION, filter=delete_expr)
        
        # 2. åˆ é™¤æœ¬åœ°å…ƒæ•°æ®
        doc_info = self.documents.pop(doc_id)
        self._save_local_metadata()
        
        # 3. åˆ é™¤ç‰©ç†æ–‡ä»¶ (å¯é€‰)
        path = Path(doc_info['path'])
        if path.exists():
            path.unlink()

        logger.info(f"ðŸ—‘ï¸ æ–‡æ¡£å·²åˆ é™¤: {doc_id}")

    def list_documents(self) -> List[Dict]:
        return list(self.documents.values())

    def get_stats(self):
        total_docs = len(self.documents)
        # Milvus ç»Ÿè®¡è¡Œæ•°ä¼°ç®—
        res = self.milvus.query(collection_name=self.settings.MILVUS_COLLECTION, filter="", output_fields=["count(*)"])
        total_chunks = res[0]["count(*)"] if res else 0
        return {
            "index_status": "Indexed" if total_chunks > 0 else "Empty",
            "total_documents": total_docs,
            "total_chunks": total_chunks
        }

    async def query(self, question: str, top_k: int = 5) -> str:
        """å‘é‡æ£€ç´¢ + LLM ç”Ÿæˆ"""
        # 1. Embedding
        q_vec = (await self._get_embedding([question]))[0]

        # 2. Milvus Search
        results = self.milvus.search(
            collection_name=self.settings.MILVUS_COLLECTION,
            data=[q_vec],
            limit=top_k,
            output_fields=["text", "source"]
        )

        if not results or not results[0]:
            return "æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£ä¿¡æ¯ã€‚"

        # 3. æž„é€  Context
        retrieved_texts = [hit['entity']['text'] for hit in results[0]]
        context_str = "\n\n---\n\n".join(retrieved_texts)

        return context_str

        # # 4. LLM Generate
        # system_prompt = "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ã€‚è¯·æ ¹æ®æä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯å›žç­”ç”¨æˆ·çš„é—®é¢˜ã€‚å¦‚æžœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç­”æ¡ˆï¼Œè¯·è¯šå®žå‘ŠçŸ¥ã€‚"
        # user_prompt = f"ä¸Šä¸‹æ–‡:\n{context_str}\n\né—®é¢˜: {question}"

        # resp = await self.client_llm.chat.completions.create(
        #     model=self.settings.LLM_MODEL,
        #     messages=[
        #         {"role": "system", "content": system_prompt},
        #         {"role": "user", "content": user_prompt}
        #     ]
        # )
        # return resp.choices[0].message.content




    def save_upload_file(self, file: UploadFile) -> Path:
        file_path = self.settings.UPLOAD_DIR / file.filename
        with file_path.open("wb") as buffer:
            shutil.copyfileobj(file.file, buffer)
        return file_path
