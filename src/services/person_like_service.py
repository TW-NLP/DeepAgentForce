"""
ç”¨æˆ·åå¥½æŒ–æ˜ç³»ç»Ÿ
1. LLM æå–å¯¹è¯ä¸­çš„å®ä½“å’Œå…³ç³»
2. NetworkX æ„å»ºçŸ¥è¯†å›¾è°±
3. å›¾ç®—æ³•ï¼ˆPageRankã€ä¸­å¿ƒæ€§ç­‰ï¼‰æŒ–æ˜ç”¨æˆ·åå¥½
"""

import json
import os
from langchain_openai import ChatOpenAI
import networkx as nx
from typing import List, Dict, Tuple



class UserPreferenceMining():
    """ç”¨æˆ·åå¥½æŒ–æ˜,å®šæ—¶ä»»åŠ¡"""
    
    def __init__(self,settings):
        self.settings = settings
        self.client = ChatOpenAI(base_url=settings.LLM_URL, api_key=settings.LLM_API_KEY, model=settings.LLM_MODEL)

        self.graph = nx.DiGraph()
        
        # æ·»åŠ ç”¨æˆ·ä¸­å¿ƒèŠ‚ç‚¹
        self.graph.add_node('USER', type='user', label='ç”¨æˆ·')

        
    
    def extract_entities_relations(self, conversations: List[Dict]) -> List[Dict]:
        """
        æ­¥éª¤1: ä½¿ç”¨ LLM ä»å¯¹è¯ä¸­æå–å®ä½“å’Œå…³ç³»
        
        Args:
            conversations: å¯¹è¯åˆ—è¡¨
            
        Returns:
            æå–çš„å®ä½“å…³ç³»åˆ—è¡¨
        """
        # æ„å»ºå¯¹è¯æ–‡æœ¬
        conv_text = "\n".join(conversations)
        
        prompt = f"""ä»ç”¨æˆ·çš„å¯¹è¯ä¸­æå–å®ä½“å’Œå…³ç³»ï¼Œç”¨äºæ„å»ºçŸ¥è¯†å›¾è°±ã€‚

å¯¹è¯è®°å½•ï¼š
{conv_text}

è¯·æå–ï¼š
1. å®ä½“ï¼ˆentitiesï¼‰ï¼šç”¨æˆ·æåˆ°çš„å…·ä½“äº‹ç‰©ã€æ¦‚å¿µã€è¯é¢˜ç­‰
2. å…³ç³»ï¼ˆrelationsï¼‰ï¼šç”¨æˆ·ä¸å®ä½“ä¹‹é—´çš„å…³ç³»

ä»¥ JSON æ ¼å¼è¿”å›ï¼š
{{
  "entities": [
    {{"name": "Python", "type": "æŠ€æœ¯", "mentions": 3}},
    {{"name": "æ•°æ®åˆ†æ", "type": "é¢†åŸŸ", "mentions": 2}}
  ],
  "relations": [
    {{"source": "USER", "target": "Python", "relation": "æ„Ÿå…´è¶£", "weight": 0.9}},
    {{"source": "USER", "target": "æ•°æ®åˆ†æ", "relation": "æƒ³å­¦ä¹ ", "weight": 0.8}}
  ]
}}

æ³¨æ„ï¼š
- å®ä½“ name è¦ç®€æ´æ˜ç¡®
- å…³ç³» source ç»Ÿä¸€ç”¨ "USER" ä»£è¡¨ç”¨æˆ·
- weight èŒƒå›´ 0-1ï¼Œè¡¨ç¤ºå…³ç³»å¼ºåº¦
- åªæå–æ˜ç¡®çš„ã€æœ‰ä»·å€¼çš„å®ä½“å’Œå…³ç³»

åªè¿”å› JSONï¼Œæ— å…¶ä»–å†…å®¹ã€‚"""

        print("ğŸ¤– LLM æå–å®ä½“å’Œå…³ç³»...")
        response = self.client.invoke(prompt)
        
        result_text = response.content
        result_text = result_text.replace('```json', '').replace('```', '').strip()
        
        extracted = json.loads(result_text)
        print(f"âœ“ æå–åˆ° {len(extracted['entities'])} ä¸ªå®ä½“")
        print(f"âœ“ æå–åˆ° {len(extracted['relations'])} ä¸ªå…³ç³»")
        
        return extracted
    
    def build_knowledge_graph(self, extracted_data: Dict):
        """
        æ­¥éª¤2: ä½¿ç”¨ NetworkX æ„å»ºçŸ¥è¯†å›¾è°±
        
        Args:
            extracted_data: LLM æå–çš„å®ä½“å’Œå…³ç³»
        """
        print("\nğŸ“Š æ„å»ºçŸ¥è¯†å›¾è°±...")
        
        # æ·»åŠ å®ä½“èŠ‚ç‚¹
        for entity in extracted_data['entities']:
            self.graph.add_node(
                entity['name'],
                type='entity',
                entity_type=entity.get('type', 'unknown'),
                mentions=entity.get('mentions', 1),
                label=entity['name']
            )
        
        # æ·»åŠ å…³ç³»è¾¹
        for relation in extracted_data['relations']:
            self.graph.add_edge(
                relation['source'],
                relation['target'],
                relation=relation['relation'],
                weight=relation.get('weight', 0.5)
            )
        
        print(f"âœ“ å›¾è°±èŠ‚ç‚¹æ•°: {self.graph.number_of_nodes()}")
        print(f"âœ“ å›¾è°±è¾¹æ•°: {self.graph.number_of_edges()}")
    
    def mine_preferences_with_graph_algorithms(self) -> Dict:
        """
        æ­¥éª¤3: ä½¿ç”¨å›¾ç®—æ³•æŒ–æ˜ç”¨æˆ·åå¥½
        
        ä½¿ç”¨çš„ç®—æ³•ï¼š
        - PageRank: è®¡ç®—èŠ‚ç‚¹é‡è¦æ€§
        - Degree Centrality: åº¦ä¸­å¿ƒæ€§ï¼ˆè¿æ¥æ•°ï¼‰
        - Betweenness Centrality: ä»‹æ•°ä¸­å¿ƒæ€§
        """
        print("\nğŸ” ä½¿ç”¨å›¾ç®—æ³•æŒ–æ˜åå¥½...")

        
        # 1. PageRank - è®¡ç®—æ¯ä¸ªå®ä½“çš„é‡è¦æ€§
        pagerank_scores = nx.pagerank(self.graph, weight='weight')
        # æ’é™¤ç”¨æˆ·èŠ‚ç‚¹ï¼Œåªçœ‹å®ä½“
        entity_pagerank = {
            node: score 
            for node, score in pagerank_scores.items() 
            if node != 'USER'
        }
        top_pagerank = sorted(entity_pagerank.items(), key=lambda x: x[1], reverse=True)[:10]
        
        # 2. åº¦ä¸­å¿ƒæ€§ - ç”¨æˆ·ç›´æ¥è¿æ¥çš„å®ä½“
        user_neighbors = list(self.graph.successors('USER'))
        neighbor_weights = {}
        for neighbor in user_neighbors:
            edge_data = self.graph['USER'][neighbor]
            neighbor_weights[neighbor] = edge_data.get('weight', 0.5)
        
        top_neighbors = sorted(neighbor_weights.items(), key=lambda x: x[1], reverse=True)
        
        # 3. èŠ‚ç‚¹å±æ€§åˆ†æ
        entity_mentions = {}
        entity_types = {}
        for node, data in self.graph.nodes(data=True):
            if data.get('type') == 'entity':
                entity_mentions[node] = data.get('mentions', 0)
                entity_types[node] = data.get('entity_type', 'unknown')
        
        top_mentions = sorted(entity_mentions.items(), key=lambda x: x[1], reverse=True)[:10]
        
        # 4. ç»¼åˆè®¡ç®—åå¥½å¾—åˆ†
        preference_scores = {}
        for entity in entity_pagerank.keys():
            score = 0.0
            
            # PageRank æƒé‡ (40%)
            score += pagerank_scores.get(entity, 0) * 40
            
            # ç›´æ¥è¿æ¥æƒé‡ (30%)
            if entity in neighbor_weights:
                score += neighbor_weights[entity] * 30
            
            # æåŠæ¬¡æ•°æƒé‡ (30%)
            mentions = entity_mentions.get(entity, 0)
            max_mentions = max(entity_mentions.values()) if entity_mentions else 1
            score += (mentions / max_mentions) * 30
            
            preference_scores[entity] = score
        
        top_preferences = sorted(preference_scores.items(), key=lambda x: x[1], reverse=True)[:10]
        
        # æ•´ç†ç»“æœ
        result = {
            "top_preferences": [
                {
                    "entity": entity,
                    "score": round(score, 3),
                    "type": entity_types.get(entity, 'unknown'),
                    "mentions": entity_mentions.get(entity, 0),
                    "pagerank": round(pagerank_scores.get(entity, 0), 4)
                }
                for entity, score in top_preferences
            ],
            "algorithm_results": {
                "pagerank_top10": [
                    {"entity": e, "score": round(s, 4)} 
                    for e, s in top_pagerank
                ],
                "direct_connections": [
                    {"entity": e, "weight": round(w, 3)}
                    for e, w in top_neighbors
                ],
                "most_mentioned": [
                    {"entity": e, "mentions": m}
                    for e, m in top_mentions
                ]
            },
            "graph_statistics": {
                "total_nodes": self.graph.number_of_nodes(),
                "total_edges": self.graph.number_of_edges(),
                "user_connections": self.graph.out_degree('USER'),
                "avg_clustering": round(nx.average_clustering(self.graph.to_undirected()), 3)
            }
        }
        
        return result
    
    def save_graph(self, filepath: str = 'knowledge_graph.json'):
        """ä¿å­˜çŸ¥è¯†å›¾è°±"""
        graph_data = {
            'nodes': [
                {'id': node, **data}
                for node, data in self.graph.nodes(data=True)
            ],
            'edges': [
                {'source': u, 'target': v, **data}
                for u, v, data in self.graph.edges(data=True)
            ]
        }
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(graph_data, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ çŸ¥è¯†å›¾è°±å·²ä¿å­˜: {filepath}")

    def get_frontend_format(self) -> Dict:
        return json.loads(open(self.settings.PERSON_LIKE_FILE, 'r', encoding='utf-8').read())
    def person_like_save(self) -> Dict:
        """
        è¿”å›ç¬¦åˆå‰ç«¯è¦æ±‚çš„æ•°æ®æ ¼å¼
        ç”¨äº /get/person_like æ¥å£
        """
        # è·å–session æ•°æ®
        sessions_list=[]
        for file_i in os.listdir(self.settings.HISTORY_DIR):
            file_path = os.path.join(self.settings.HISTORY_DIR, file_i)
            with open(file_path, 'r', encoding='utf-8') as f:
                file_data=json.load(f)
                sessions_list.extend([i.get("user_content", "") for i in file_data.get('conversations', [])])
        sessions_list = [item for item in sessions_list if item.strip()]

        # æ­¥éª¤1: LLM æå–å®ä½“å’Œå…³ç³»
        extracted = self.extract_entities_relations(sessions_list)
        # æ­¥éª¤2: æ„å»ºçŸ¥è¯†å›¾è°±
        self.build_knowledge_graph(extracted)
        # æŒ–æ˜åå¥½
        preferences_result = self.mine_preferences_with_graph_algorithms()
        
        # å›¾è°±æ•°æ®
        graph_data = {
            'nodes': [
                {
                    'id': node,
                    'type': data.get('type', 'unknown'),
                    'label': data.get('label', node)
                }
                for node, data in self.graph.nodes(data=True)
            ],
            'edges': [
                {
                    'source': u,
                    'target': v,
                    'relation': data.get('relation', ''),
                    'weight': data.get('weight', 0.5)
                }
                for u, v, data in self.graph.edges(data=True)
            ]
        }
        api_response = {
            'graph': graph_data,
            'preferences': preferences_result['top_preferences'],
            'statistics': preferences_result['graph_statistics'],
        }

        prompt = f"""
è¯·æ ¹æ®ä»¥ä¸‹â€œç”¨æˆ·åå¥½æŒ–æ˜ç³»ç»Ÿâ€çš„è¾“å‡ºæ•°æ®ï¼Œç”Ÿæˆä¸€æ®µç®€ç»ƒã€ä¸“ä¸šçš„ã€ç”¨æˆ·ç”»åƒä¾§å†™ã€‘ã€‚

ã€è¾“å…¥æ•°æ®ã€‘ï¼š
{json.dumps(api_response, ensure_ascii=False, indent=2)}

ã€è¦æ±‚ã€‘ï¼š
1. **æ ¸å¿ƒå®šä½**ï¼šä¸€å¥è¯æ¦‚æ‹¬ç”¨æˆ·çš„æ ¸å¿ƒèº«ä»½ï¼ˆä¾‹å¦‚ï¼šAIæ–¹å‘çš„å¼€å‘è€…ã€Pythonåˆå­¦è€…ç­‰ï¼‰ã€‚
2. **åå¥½è§£è¯»**ï¼šç»“åˆ `preferences` ä¸­çš„åˆ†æ•°æ’åï¼Œè¯´æ˜ç”¨æˆ·æœ€å…³æ³¨çš„é¢†åŸŸæˆ–å·¥å…·ã€‚
3. **å…³ç³»ç»†èŠ‚**ï¼šåˆ©ç”¨ `edges` ä¸­çš„ `relation` å­—æ®µï¼ˆå¦‚â€œæ„Ÿå…´è¶£â€vsâ€œåå¥½ä½¿ç”¨â€ï¼‰æ¥åŒºåˆ†ç”¨æˆ·æ˜¯å•çº¯æ„Ÿå…´è¶£è¿˜æ˜¯æœ‰å®æ“éœ€æ±‚ã€‚
4. **å£å»**ï¼šå®¢è§‚ã€ä¸“ä¸šï¼Œç±»ä¼¼äºCRMç³»ç»Ÿä¸­çš„ç”¨æˆ·å¤‡æ³¨ã€‚
5. **å­—æ•°**ï¼š150å­—ä»¥å†…ã€‚
"""
        api_response['summary']=self.client.invoke(prompt).content.strip()
        with open(self.settings.PERSON_LIKE_FILE, 'w', encoding='utf-8') as f:
            json.dump(api_response, f, ensure_ascii=False, indent=2)
    
    def visualize_graph(self, output_path: str = 'preference_graph.png'):
        """å¯è§†åŒ–çŸ¥è¯†å›¾è°±"""
        try:
            import matplotlib.pyplot as plt
            
            plt.figure(figsize=(14, 10))
            
            # å¸ƒå±€
            pos = nx.spring_layout(self.graph, k=2, iterations=50, seed=42)
            
            # èŠ‚ç‚¹åˆ†ç»„
            user_nodes = [n for n, d in self.graph.nodes(data=True) if d.get('type') == 'user']
            entity_nodes = [n for n, d in self.graph.nodes(data=True) if d.get('type') == 'entity']
            
            # ç»˜åˆ¶èŠ‚ç‚¹
            nx.draw_networkx_nodes(self.graph, pos, nodelist=user_nodes, 
                                  node_color='#FF6B6B', node_size=3000, alpha=0.9, label='ç”¨æˆ·')
            nx.draw_networkx_nodes(self.graph, pos, nodelist=entity_nodes,
                                  node_color='#4ECDC4', node_size=1500, alpha=0.8, label='å®ä½“')
            
            # ç»˜åˆ¶è¾¹
            nx.draw_networkx_edges(self.graph, pos, edge_color='gray', 
                                  alpha=0.3, arrows=True, arrowsize=20, width=2)
            
            # ç»˜åˆ¶æ ‡ç­¾
            nx.draw_networkx_labels(self.graph, pos, font_size=9, font_family='sans-serif')
            
            plt.title('ç”¨æˆ·åå¥½çŸ¥è¯†å›¾è°±', fontsize=16, fontweight='bold')
            plt.legend(loc='upper left', fontsize=11)
            plt.axis('off')
            plt.tight_layout()
            plt.savefig(output_path, dpi=300, bbox_inches='tight', facecolor='white')
            plt.close()
            
            print(f"ğŸ¨ å›¾è°±å¯è§†åŒ–å·²ä¿å­˜: {output_path}")
        except Exception as e:
            print(f"å¯è§†åŒ–å¤±è´¥: {e}")

