"""
æœç´¢æœåŠ¡æ¨¡å—
å°è£… Tavily å’Œ Firecrawl çš„è°ƒç”¨
"""

import json
import logging
from typing import Optional, List, Dict, Any
from tavily import TavilyClient
from firecrawl import FirecrawlApp
from config.settings import settings

logger = logging.getLogger(__name__)


class SearchService:
    """æœç´¢æœåŠ¡ç±»"""
    
    def __init__(self):
        """åˆå§‹åŒ–æœç´¢æœåŠ¡"""
        self.tavily_client = TavilyClient(api_key=settings.TAVILY_API_KEY)
        self.firecrawl_client = FirecrawlApp(api_key=settings.FIRECRAWL_API_KEY) if settings.FIRECRAWL_API_KEY else None
        
        logger.info("âœ… æœç´¢æœåŠ¡åˆå§‹åŒ–æˆåŠŸ")
    
    async def web_search(
        self,
        query: str,
    ) -> Dict[str, Any]:
        """
        æ‰§è¡Œç½‘ç»œæœç´¢
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            max_results: æœ€å¤§ç»“æœæ•°
            
        Returns:
            æœç´¢ç»“æœå­—å…¸
        """
        try:
            
            logger.info(f"ğŸ” æ‰§è¡Œç½‘ç»œæœç´¢: {query}")
            
            response = self.tavily_client.search(
                query=query
            )
            
            logger.info(f"âœ… æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(response.get('results', []))} ä¸ªç»“æœï¼Œå†…å®¹å¦‚ä¸‹ï¼š{json.dumps(response, ensure_ascii=False)}")
            return response
            
        except Exception as e:
            logger.error(f"âŒ ç½‘ç»œæœç´¢å¤±è´¥: {e}")
            raise
    
    async def crawl_url(
        self,
        url: str,
        max_length: Optional[int] = None
    ) -> Dict[str, Any]:
        """
        çˆ¬å–å•ä¸ªç½‘é¡µ
        
        Args:
            url: ç½‘é¡µ URL
            max_length: å†…å®¹æœ€å¤§é•¿åº¦
            
        Returns:
            çˆ¬å–ç»“æœï¼ŒåŒ…å« markdown æ ¼å¼çš„å†…å®¹
        """
        try:
            if not self.firecrawl_client:
                return ''
            logger.info(f"ğŸ•·ï¸ çˆ¬å–ç½‘é¡µ: {url}")
            
            result = self.firecrawl_client.scrape(url)
            
            # é™åˆ¶å†…å®¹é•¿åº¦
            max_len = max_length or settings.FIRECRAWL_MAX_CONTENT_LENGTH
            if hasattr(result, 'markdown') and result.markdown:
                result.markdown = result.markdown[:max_len]
            
            logger.info(f"âœ… çˆ¬å–æˆåŠŸ: {url}")
            return result
            
        except Exception as e:
            logger.warning(f"âš ï¸ çˆ¬å–å¤±è´¥ {url}: {e}")
            raise
    
    async def crawl_multiple_urls(
        self,
        urls: List[str],
        max_length: Optional[int] = None
    ) -> List[Dict[str, Any]]:
        """
        å¹¶è¡Œçˆ¬å–å¤šä¸ªç½‘é¡µ
        
        Args:
            urls: URL åˆ—è¡¨
            max_length: å†…å®¹æœ€å¤§é•¿åº¦
            
        Returns:
            çˆ¬å–ç»“æœåˆ—è¡¨
        """
        results = []
        
        for url in urls[:settings.MAX_URLS_TO_CRAWL]:
            try:
                result = await self.crawl_url(url, max_length)
                results.append({
                    "url": url,
                    "success": True,
                    "content": result
                })
            except Exception as e:
                logger.warning(f"è·³è¿‡çˆ¬å–å¤±è´¥çš„ URL: {url}")
                results.append({
                    "url": url,
                    "success": False,
                    "error": str(e)
                })
        
        success_count = sum(1 for r in results if r.get("success"))
        logger.info(f"ğŸ“Š æ‰¹é‡çˆ¬å–å®Œæˆ: {success_count}/{len(urls)} æˆåŠŸï¼Œå†…å®¹å¦‚ä¸‹ï¼š{results}")
        
        return results
    
    async def search_and_crawl(
        self,
        query: str,
        num_urls: Optional[int] = None
    ) -> Dict[str, Any]:
        """
        ä¸€ç«™å¼æœç´¢+çˆ¬å–
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            num_urls: è¦çˆ¬å–çš„ URL æ•°é‡
            
        Returns:
            åŒ…å«æœç´¢ç»“æœå’Œçˆ¬å–å†…å®¹çš„å­—å…¸
        """
        # æ‰§è¡Œæœç´¢
        search_results = await self.web_search(query)
        
        # æå– URL
        urls = [
            result['url'] 
            for result in search_results.get('results', [])
        ][:num_urls or settings.MAX_URLS_TO_CRAWL]
        
        # çˆ¬å–å†…å®¹
        crawled_results = await self.crawl_multiple_urls(urls)
        logger.info(f"æœç´¢çš„å…¨éƒ¨å†…å®¹å¦‚ä¸‹ï¼š{
            {
            "query": query,
            "search_results": search_results,
            "crawled_flag": True if self.firecrawl_client else False,
            "crawled_contents": [
                {
                    "url": r["url"],
                    "title": next(
                        (res['title'] for res in search_results.get('results', []) 
                         if res['url'] == r["url"]),
                        r["url"]
                    ),
                    "snippet": next(
                        (res['content'] for res in search_results.get('results', []) 
                         if res['url'] == r["url"]),
                        ""
                    ),
                    "full_content": r["content"].markdown[:settings.FIRECRAWL_MAX_CONTENT_LENGTH]
                    if r.get("success") and hasattr(r["content"], 'markdown')
                    else ""
                }
                for r in crawled_results
                if r.get("success")
            ]
        }
        }")
        # ç»„åˆç»“æœ
        return {
            "query": query,
            "search_results": search_results,
            "crawled_contents": [
                {
                    "url": r["url"],
                    "title": next(
                        (res['title'] for res in search_results.get('results', []) 
                         if res['url'] == r["url"]),
                        r["url"]
                    ),
                    "snippet": next(
                        (res['content'] for res in search_results.get('results', []) 
                         if res['url'] == r["url"]),
                        ""
                    ),
                    "full_content": r["content"].markdown[:settings.FIRECRAWL_MAX_CONTENT_LENGTH]
                    if r.get("success") and hasattr(r["content"], 'markdown')
                    else ""
                }
                for r in crawled_results
                if r.get("success")
            ]
        }


# åˆ›å»ºå…¨å±€æœç´¢æœåŠ¡å®ä¾‹
_search_service: Optional[SearchService] = None


def get_search_service() -> SearchService:
    """è·å–æœç´¢æœåŠ¡å•ä¾‹"""
    global _search_service
    if _search_service is None:
        _search_service = SearchService()
    return _search_service