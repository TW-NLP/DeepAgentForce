import asyncio
import httpx
import faiss
import numpy as np
from typing import List, Dict, Optional, Set, Tuple
import networkx as nx
from collections import defaultdict
import tiktoken
from community import community_louvain
import json
import pickle
from pathlib import Path
from datetime import datetime
import hashlib
import uuid
import logging
from dataclasses import dataclass
from difflib import SequenceMatcher
from pypdf import PdfReader
import pdfplumber
from docx import Document as DocxDocument
import pandas as pd
import re

logger = logging.getLogger(__name__)


@dataclass
class EntityAlignment:
    """å®ä½“å¯¹é½ç»“æœ"""
    canonical_name: str  # æ ‡å‡†åç§°
    aliases: List[str]   # åˆ«ååˆ—è¡¨
    similarity: float    # ç›¸ä¼¼åº¦


class AsyncLLMClient:
    """å¼‚æ­¥ LLM å®¢æˆ·ç«¯"""
    
    def __init__(self, base_url: str, api_key: str, model: str, max_concurrent: int = 10):
        self.base_url = base_url.rstrip('/')
        self.api_key = api_key
        self.model = model
        self.semaphore = asyncio.Semaphore(max_concurrent)
        
        # ä½¿ç”¨è¿æ¥æ± 
        self.client = httpx.AsyncClient(
            timeout=httpx.Timeout(60.0, connect=10.0),
            limits=httpx.Limits(max_keepalive_connections=20, max_connections=100)
        )
    
    async def chat(self, messages: List[Dict], temperature: float = 0, 
                   max_tokens: int = 10000, response_format: Optional[Dict] = None) -> str:
        """å¼‚æ­¥èŠå¤©è¡¥å…¨"""
        async with self.semaphore:
            payload = {
                "model": self.model,
                "messages": messages,
                "temperature": temperature,
                "max_tokens": max_tokens
            }
            
            if response_format:
                payload["response_format"] = response_format
            
            try:
                response = await self.client.post(
                    f"{self.base_url}/chat/completions",
                    headers={
                        "Authorization": f"Bearer {self.api_key}",
                        "Content-Type": "application/json"
                    },
                    json=payload
                )
                response.raise_for_status()
                result = response.json()
                return result['choices'][0]['message']['content']
            except Exception as e:
                logger.error(f"LLM è°ƒç”¨å¤±è´¥: {e}")
                raise
    
    async def close(self):
        """å…³é—­å®¢æˆ·ç«¯"""
        await self.client.aclose()


class AsyncEmbeddingClient:
    """å¼‚æ­¥ Embedding å®¢æˆ·ç«¯"""
    
    def __init__(self, base_url: str, api_key: str, model: str, max_concurrent: int = 20):
        self.base_url = base_url.rstrip('/')
        self.api_key = api_key
        self.model = model
        self.semaphore = asyncio.Semaphore(max_concurrent)
        
        self.client = httpx.AsyncClient(
            timeout=httpx.Timeout(60.0, connect=10.0),
            limits=httpx.Limits(max_keepalive_connections=20, max_connections=100)
        )
    
    async def embed(self, texts: List[str]) -> List[np.ndarray]:
        """æ‰¹é‡ç”Ÿæˆ embeddings"""
        async with self.semaphore:
            try:
                response = await self.client.post(
                    f"{self.base_url}/embeddings",
                    headers={
                        "Authorization": f"Bearer {self.api_key}",
                        "Content-Type": "application/json"
                    },
                    json={
                        "model": self.model,
                        "input": texts
                    }
                )
                response.raise_for_status()
                result = response.json()
                return [np.array(item['embedding'], dtype='float32') 
                       for item in result['data']]
            except Exception as e:
                logger.error(f"Embedding è°ƒç”¨å¤±è´¥: {e}")
                raise
    
    async def close(self):
        """å…³é—­å®¢æˆ·ç«¯"""
        await self.client.aclose()


class DocumentParser:
    """æ–‡æ¡£è§£æå™¨ (ä¿æŒåŒæ­¥ï¼ŒIO å¯†é›†å‹)"""
    
    @staticmethod
    def parse_pdf(file_path: str) -> str:
        """è§£æ PDF æ–‡ä»¶"""
        try:
            with pdfplumber.open(file_path) as pdf:
                text_parts = []
                for page_num, page in enumerate(pdf.pages, 1):
                    page_text = page.extract_text()
                    if page_text:
                        text_parts.append(f"[Page {page_num}]\n{page_text}")
                    
                    tables = page.extract_tables()
                    for table_num, table in enumerate(tables, 1):
                        if table:
                            table_text = f"\n[Table {table_num} on Page {page_num}]\n"
                            for row in table:
                                table_text += " | ".join([str(cell) if cell else "" for cell in row]) + "\n"
                            text_parts.append(table_text)
                
                return "\n\n".join(text_parts)
        except Exception as e:
            logger.warning(f"pdfplumber è§£æå¤±è´¥ï¼Œä½¿ç”¨ pypdf: {e}")
            reader = PdfReader(file_path)
            text_parts = []
            for page_num, page in enumerate(reader.pages, 1):
                text = page.extract_text()
                if text:
                    text_parts.append(f"[Page {page_num}]\n{text}")
            return "\n\n".join(text_parts)
    
    @staticmethod
    def parse_docx(file_path: str) -> str:
        """è§£æ DOCX æ–‡ä»¶"""
        doc = DocxDocument(file_path)
        text_parts = []
        
        for para in doc.paragraphs:
            if para.text.strip():
                text_parts.append(para.text)
        
        for table_num, table in enumerate(doc.tables, 1):
            table_text = f"\n[Table {table_num}]\n"
            for row in table.rows:
                row_text = " | ".join([cell.text for cell in row.cells])
                table_text += row_text + "\n"
            text_parts.append(table_text)
        
        return "\n\n".join(text_parts)
    
    @staticmethod
    def parse_txt(file_path: str) -> str:
        """è§£æçº¯æ–‡æœ¬æ–‡ä»¶"""
        with open(file_path, 'r', encoding='utf-8') as f:
            return f.read()
    
    @staticmethod
    def parse_markdown(file_path: str) -> str:
        """è§£æ Markdown æ–‡ä»¶"""
        with open(file_path, 'r', encoding='utf-8') as f:
            return f.read()
    
    @staticmethod
    def parse_csv(file_path: str) -> str:
        """è§£æ CSV æ–‡ä»¶"""
        df = pd.read_csv(file_path)
        text = f"CSV Data ({len(df)} rows x {len(df.columns)} columns)\n\n"
        text += df.to_string(index=False)
        return text
    
    @classmethod
    def parse_document(cls, file_path: str) -> str:
        """æ ¹æ®æ–‡ä»¶æ‰©å±•åè‡ªåŠ¨é€‰æ‹©è§£æå™¨"""
        path = Path(file_path)
        extension = path.suffix.lower()
        
        parsers = {
            '.pdf': cls.parse_pdf,
            '.docx': cls.parse_docx,
            '.doc': cls.parse_docx,
            '.txt': cls.parse_txt,
            '.md': cls.parse_markdown,
            '.markdown': cls.parse_markdown,
            '.csv': cls.parse_csv,
        }
        
        parser = parsers.get(extension)
        if parser is None:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {extension}")
        
        return parser(str(path))


class TextChunker:
    """æ™ºèƒ½æ–‡æœ¬åˆ†å—å™¨"""
    
    def __init__(self, chunk_size: int = 600, overlap: int = 100):
        self.chunk_size = chunk_size
        self.overlap = overlap
        self.encoding = tiktoken.get_encoding("cl100k_base")
    
    def chunk_by_sentences(self, text: str) -> List[str]:
        """æŒ‰å¥å­åˆ†å—ï¼ˆä¿æŒå¥å­å®Œæ•´æ€§ï¼‰"""
        sentences = re.split(r'[.!?ã€‚ï¼ï¼Ÿ]\s+', text)
        
        chunks = []
        current_chunk = []
        current_length = 0
        
        for sentence in sentences:
            sentence_words = len(sentence.split())
            
            if current_length + sentence_words > self.chunk_size and current_chunk:
                chunks.append(' '.join(current_chunk))
                
                overlap_sentences = []
                overlap_length = 0
                for s in reversed(current_chunk):
                    s_length = len(s.split())
                    if overlap_length + s_length <= self.overlap:
                        overlap_sentences.insert(0, s)
                        overlap_length += s_length
                    else:
                        break
                
                current_chunk = overlap_sentences
                current_length = overlap_length
            
            current_chunk.append(sentence)
            current_length += sentence_words
        
        if current_chunk:
            chunks.append(' '.join(current_chunk))
        
        return chunks
    
    def chunk_text(self, text: str) -> List[str]:
        """åˆ†å—æ–‡æœ¬"""
        return self.chunk_by_sentences(text)


class EntityAligner:
    """å®ä½“å¯¹é½å™¨ - åˆå¹¶ç›¸ä¼¼å®ä½“"""
    
    def __init__(self, similarity_threshold: float = 0.85):
        self.similarity_threshold = similarity_threshold
    
    def calculate_similarity(self, name1: str, name2: str) -> float:
        """è®¡ç®—ä¸¤ä¸ªå®ä½“åçš„ç›¸ä¼¼åº¦"""
        # 1. å®Œå…¨åŒ¹é…
        if name1.lower() == name2.lower():
            return 1.0
        
        # 2. å­—ç¬¦ä¸²ç›¸ä¼¼åº¦
        seq_similarity = SequenceMatcher(None, name1.lower(), name2.lower()).ratio()
        
        # 3. åŒ…å«å…³ç³»
        if name1.lower() in name2.lower() or name2.lower() in name1.lower():
            return max(seq_similarity, 0.9)
        
        # 4. è¯é›†ç›¸ä¼¼åº¦ (Jaccard)
        words1 = set(name1.lower().split())
        words2 = set(name2.lower().split())
        if words1 and words2:
            jaccard = len(words1 & words2) / len(words1 | words2)
            return max(seq_similarity, jaccard)
        
        return seq_similarity
    
    def align_entities(self, entities: Dict[str, Dict]) -> Dict[str, EntityAlignment]:
        """å¯¹é½å®ä½“ï¼Œè¿”å›æ˜ å°„å…³ç³»"""
        entity_names = list(entities.keys())
        alignments = {}
        processed = set()
        
        for i, name1 in enumerate(entity_names):
            if name1 in processed:
                continue
            
            # æŸ¥æ‰¾ç›¸ä¼¼å®ä½“
            similar_entities = [name1]
            
            for name2 in entity_names[i+1:]:
                if name2 in processed:
                    continue
                
                similarity = self.calculate_similarity(name1, name2)
                
                if similarity >= self.similarity_threshold:
                    similar_entities.append(name2)
                    processed.add(name2)
            
            # é€‰æ‹©æœ€å…·ä»£è¡¨æ€§çš„åç§°ä½œä¸ºæ ‡å‡†å
            canonical_name = max(similar_entities, key=len)  # é€‰æœ€é•¿çš„
            
            alignment = EntityAlignment(
                canonical_name=canonical_name,
                aliases=similar_entities,
                similarity=1.0
            )
            
            for entity_name in similar_entities:
                alignments[entity_name] = alignment
            
            processed.add(name1)
        
        return alignments


class GraphRAGPipeline:
    """
    å¼‚æ­¥ GraphRAG Pipeline
    
    ä¼˜åŒ–:
    1. å…¨é¢å¼‚æ­¥åŒ–
    2. å¢é‡æ„å»ºï¼šæ–°æ–‡æ¡£åªè§¦å‘å±€éƒ¨æ›´æ–°
    3. å®ä½“å¯¹é½ï¼šæ™ºèƒ½åˆå¹¶ç›¸ä¼¼å®ä½“
    4. æ‰¹é‡å¹¶å‘å¤„ç†
    """

    def __init__(self, llm_api_key: str, embedding_api_key: str, llm_url: str, 
                 embedding_url: str, embedding_name: str, embedding_dim: int,
                 llm_name: str, storage_dir: str = "./graphrag_storage",
                 max_llm_concurrent: int = 10, max_embed_concurrent: int = 20):
        
        self.llm_client = AsyncLLMClient(llm_url, llm_api_key, llm_name, max_llm_concurrent)
        self.embedding_client = AsyncEmbeddingClient(embedding_url, embedding_api_key, 
                                                     embedding_name, max_embed_concurrent)
        
        self.embedding_name = embedding_name
        self.llm_name = llm_name
        self.dimension = embedding_dim
        
        # å­˜å‚¨ç›®å½•
        self.storage_dir = Path(storage_dir)
        self.storage_dir.mkdir(parents=True, exist_ok=True)
        
        # æ–‡æ¡£ç®¡ç†
        self.document_parser = DocumentParser()
        self.text_chunker = TextChunker()
        self.entity_aligner = EntityAligner(similarity_threshold=0.85)
        
        self.documents = {}
        
        # UUID æ˜ å°„
        self.uuid_to_docid = {}
        self.docid_to_uuid = {}
        
        # å›¾è°±æ•°æ®
        self.text_chunks = []
        self.chunk_to_doc = {}
        self.entities = {}
        self.entity_alignments = {}  # å®ä½“å¯¹é½æ˜ å°„
        self.relationships = []
        self.claims = []
        
        # çŸ¥è¯†å›¾è°±
        self.graph = nx.Graph()
        
        # ç¤¾åŒºç»“æ„
        self.communities = {}
        self.community_summaries = {}
        
        # FAISS ç´¢å¼•
        self.community_summary_index = None
        self.community_embeddings = []
        
        self.encoding = tiktoken.get_encoding("cl100k_base")
        
        # å¢é‡æ„å»ºæ ‡è®°
        self.needs_rebuild = {
            'entities': False,
            'graph': False,
            'communities': False,
            'summaries': False,
            'index': False
        }
    
    async def initialize(self):
        """å¼‚æ­¥åˆå§‹åŒ– - è‡ªåŠ¨åŠ è½½å·²æœ‰æ•°æ®"""
        try:
            await self.load("default")
            logger.info(f"âœ… è‡ªåŠ¨åŠ è½½çŸ¥è¯†åº“æˆåŠŸ: {len(self.documents)} ä¸ªæ–‡æ¡£, {len(self.text_chunks)} ä¸ªchunks")
        except FileNotFoundError:
            logger.info("ğŸ“ æœªæ‰¾åˆ°å·²æœ‰çŸ¥è¯†åº“ï¼Œå°†åˆ›å»ºæ–°çš„")
        except Exception as e:
            logger.warning(f"âš ï¸ åŠ è½½çŸ¥è¯†åº“å¤±è´¥: {e}")
    
    # ==================== æ–‡æ¡£ç®¡ç† ====================
    
    def _calculate_file_hash(self, file_path: str) -> str:
        """è®¡ç®—æ–‡ä»¶å“ˆå¸Œ"""
        with open(file_path, 'rb') as f:
            return hashlib.md5(f.read()).hexdigest()
    
    async def add_document(self, file_path: str, metadata: Optional[Dict] = None, 
                          doc_uuid: Optional[str] = None) -> str:
        """
        å¼‚æ­¥æ·»åŠ æ–‡æ¡£ (å¢é‡æ¨¡å¼)
        
        Returns:
            æ–‡æ¡£çš„ UUID
        """
        file_path = Path(file_path)
        if not file_path.exists():
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        
        if doc_uuid is None:
            doc_uuid = str(uuid.uuid4())
        
        file_hash = self._calculate_file_hash(str(file_path))
        
        # æ£€æŸ¥æ–‡æ¡£æ˜¯å¦å·²å­˜åœ¨
        if file_hash in self.documents:
            logger.info(f"âš ï¸ æ–‡æ¡£å·²å­˜åœ¨: {file_path.name}")
            return self.docid_to_uuid.get(file_hash, doc_uuid)
        
        logger.info(f"ğŸ“„ æ·»åŠ æ–‡æ¡£: {file_path.name} (UUID: {doc_uuid})")
        
        # 1. è§£ææ–‡æ¡£ (åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œï¼Œé¿å…é˜»å¡äº‹ä»¶å¾ªç¯)
        loop = asyncio.get_event_loop()
        text = await loop.run_in_executor(None, self.document_parser.parse_document, str(file_path))
        logger.info(f"  ğŸ“ æ–‡æ¡£è§£æå®Œæˆï¼Œæ–‡æœ¬é•¿åº¦: {len(text)} å­—ç¬¦")
        
        # 2. åˆ†å—
        chunks = await loop.run_in_executor(None, self.text_chunker.chunk_text, text)
        logger.info(f"  âœ‚ï¸ åˆ†å—å®Œæˆ: {len(chunks)} ä¸ªchunks")
        
        # 3. è®°å½•æ–‡æ¡£ä¿¡æ¯
        doc_info = {
            'uuid': doc_uuid,
            'path': str(file_path),
            'name': file_path.name,
            'hash': file_hash,
            'chunks': chunks,
            'chunk_ids': [],
            'metadata': metadata or {},
            'added_at': datetime.now().isoformat()
        }
        
        self.documents[file_hash] = doc_info
        self.uuid_to_docid[doc_uuid] = file_hash
        self.docid_to_uuid[file_hash] = doc_uuid
        
        # 4. å¼‚æ­¥å¹¶å‘æå–å›¾å…ƒç´ 
        chunk_start_id = len(self.text_chunks)
        logger.info(f"  ğŸ” å¼€å§‹å¼‚æ­¥æå–å›¾å…ƒç´  (èµ·å§‹ID: {chunk_start_id})...")
        
        # å¹¶å‘æå–æ‰€æœ‰ chunks
        tasks = []
        for chunk_id, chunk in enumerate(chunks):
            global_chunk_id = chunk_start_id + chunk_id
            task = self.extract_graph_elements(chunk, global_chunk_id)
            tasks.append(task)
        
        # ç­‰å¾…æ‰€æœ‰æå–ä»»åŠ¡å®Œæˆ
        chunk_elements = await asyncio.gather(*tasks)
        
        # 5. æ·»åŠ åˆ°æ•°æ®ç»“æ„
        for chunk_id, elements in enumerate(chunk_elements):
            global_chunk_id = chunk_start_id + chunk_id
            self.text_chunks.append(elements)
            self.chunk_to_doc[global_chunk_id] = file_hash
            doc_info['chunk_ids'].append(global_chunk_id)
        
        logger.info(f"  âœ… å®Œæˆ: æå–äº† {len(chunks)} ä¸ªæ–‡æœ¬å—")
        logger.info(f"  ğŸ“Š å½“å‰æ€»è®¡: {len(self.text_chunks)} ä¸ªchunks")
        
        # 6. æ ‡è®°éœ€è¦å¢é‡æ›´æ–°
        self._mark_needs_rebuild(['entities', 'graph', 'communities', 'summaries', 'index'])
        
        # 7. è‡ªåŠ¨ä¿å­˜
        await self.save("default")
        logger.info(f"  ğŸ’¾ çŸ¥è¯†åº“å·²è‡ªåŠ¨ä¿å­˜")
        
        return doc_uuid
    
    async def remove_document(self, doc_id: str):
        """å¼‚æ­¥åˆ é™¤æ–‡æ¡£"""
        if doc_id in self.uuid_to_docid:
            internal_doc_id = self.uuid_to_docid[doc_id]
            doc_uuid = doc_id
        elif doc_id in self.documents:
            internal_doc_id = doc_id
            doc_uuid = self.docid_to_uuid.get(doc_id)
        else:
            raise ValueError(f"æ–‡æ¡£ä¸å­˜åœ¨: {doc_id}")
        
        logger.info(f"ğŸ—‘ï¸ åˆ é™¤æ–‡æ¡£: {self.documents[internal_doc_id]['name']}")
        
        # æ ‡è®°åˆ é™¤çš„ chunks
        chunk_ids = set(self.documents[internal_doc_id]['chunk_ids'])
        for chunk_id in chunk_ids:
            if chunk_id < len(self.text_chunks):
                self.text_chunks[chunk_id] = {'entities': [], 'relationships': [], 'claims': []}
            self.chunk_to_doc.pop(chunk_id, None)
        
        # åˆ é™¤æ˜ å°„
        if doc_uuid:
            self.uuid_to_docid.pop(doc_uuid, None)
            self.docid_to_uuid.pop(internal_doc_id, None)
        
        del self.documents[internal_doc_id]
        
        # æ ‡è®°éœ€è¦é‡å»º
        self._mark_needs_rebuild(['entities', 'graph', 'communities', 'summaries', 'index'])
        
        # è‡ªåŠ¨ä¿å­˜
        await self.save("default")
        logger.info("  ğŸ’¾ åˆ é™¤åå·²è‡ªåŠ¨ä¿å­˜")
        logger.info("  âœ… æ–‡æ¡£å·²åˆ é™¤")
    
    def list_documents(self) -> List[Dict]:
        """åˆ—å‡ºæ‰€æœ‰æ–‡æ¡£"""
        return [
            {
                'uuid': info['uuid'],
                'id': doc_id,
                'name': info['name'],
                'path': info['path'],
                'chunks': len(info['chunks']),
                'added_at': info['added_at'],
                'metadata': info['metadata']
            }
            for doc_id, info in self.documents.items()
        ]
    
    # ==================== å›¾å…ƒç´ æå– (å¼‚æ­¥) ====================
    @staticmethod
    def safe_json_loads(text: str):
        logger.info(f"å¤§æ¨¡å‹ç»“æœï¼š{text}")
        try:
            return json.loads(text)
        except json.JSONDecodeError:
            # 1ï¸âƒ£ å»æ‰ markdown fence
            cleaned = re.sub(r"```json|```", "", text, flags=re.I).strip()
            try:
                return json.loads(cleaned)
            except json.JSONDecodeError:
                # 2ï¸âƒ£ å°è¯•æˆªå–ç¬¬ä¸€ä¸ª { ... }
                match = re.search(r"\{.*\}", cleaned, re.S)
                if match:
                    return json.loads(match.group())
                raise

    
    async def extract_graph_elements(self, text: str, chunk_id: int) -> Dict:
        """å¼‚æ­¥æå–å›¾å…ƒç´ """
        
        prompt = f"""ä»ä»¥ä¸‹æ–‡æœ¬ä¸­æå–ç»“æ„åŒ–ä¿¡æ¯ï¼Œå¿…é¡»è¿”å›JSONæ ¼å¼ï¼Œæ ¼å¼å®Œæ•´ã€‚

æ–‡æœ¬:
{text}

æå–å†…å®¹:
1. entities: [{{"name": "å®ä½“å", "type": "ç±»å‹", "description": "æè¿°"}}]
2. relationships: [{{"source": "æºå®ä½“", "target": "ç›®æ ‡å®ä½“", "description": "å…³ç³»", "strength": 1-10}}]
3. claims: [{{"subject": "ä¸»ä½“", "object": "å®¢ä½“", "type": "FACT/OPINION", "description": "æè¿°", "date": "æ—¶é—´"}}]

åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ï¼Œæ ¼å¼å¿…é¡»å®Œæ•´ã€‚
"""

        try:
            response_text = await self.llm_client.chat(
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯çŸ¥è¯†å›¾è°±ä¸“å®¶ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0,
                response_format={"type": "json_object"}
            )
            
            result = self.safe_json_loads(response_text) 
            logger.debug(f"Chunk {chunk_id} æå–ç»“æœ: {len(result.get('entities', []))} å®ä½“")
            return result
            
        except Exception as e:
            logger.error(f"æå–å¤±è´¥ (chunk {chunk_id}): {e}")
            return {"entities": [], "relationships": [], "claims": []}
    
    async def summarize_entity(self, entity_name: str, descriptions: List[str]) -> str:
        """å¼‚æ­¥åˆå¹¶å®ä½“æè¿°"""
        if len(descriptions) == 1:
            return descriptions[0]
        
        combined = "\n".join([f"- {desc}" for desc in descriptions])
        
        prompt = f"""æ•´åˆä»¥ä¸‹å…³äº"{entity_name}"çš„æè¿°ä¸ºä¸€ä¸ªæ‘˜è¦ï¼ˆ150-200è¯ï¼‰ï¼š

{combined}

åªè¿”å›æ‘˜è¦ã€‚"""

        response_text = await self.llm_client.chat(
            messages=[{"role": "user", "content": prompt}],
            temperature=0,
            max_tokens=300
        )
        
        return response_text.strip()
    
    # ==================== å›¾è°±æ„å»º (å¼‚æ­¥ + å¢é‡) ====================
    
    def _mark_needs_rebuild(self, stages: List[str]):
        """æ ‡è®°éœ€è¦é‡å»ºçš„é˜¶æ®µ"""
        for stage in stages:
            self.needs_rebuild[stage] = True
    
    async def merge_entities_and_relationships(self, incremental: bool = True):
        """
        å¼‚æ­¥åˆå¹¶å®ä½“å’Œå…³ç³»
        
        Args:
            incremental: æ˜¯å¦å¢é‡æ¨¡å¼ï¼ˆä»…å¤„ç†æ–°æ•°æ®ï¼‰
        """
        logger.info(f"ğŸ“Š å¼€å§‹{'å¢é‡' if incremental else 'å…¨é‡'}åˆå¹¶å®ä½“å’Œå…³ç³»...")
        
        if not self.text_chunks:
            logger.warning("âš ï¸ text_chunks ä¸ºç©ºï¼è¯·å…ˆæ·»åŠ æ–‡æ¡£ã€‚")
            return
        
        # å¦‚æœæ˜¯å¢é‡æ¨¡å¼ä¸”å·²æœ‰å®ä½“ï¼Œä¿ç•™åŸæœ‰æ•°æ®
        if incremental and self.entities:
            logger.info("  ä½¿ç”¨å¢é‡æ¨¡å¼ï¼Œä¿ç•™å·²æœ‰å®ä½“")
            entity_descriptions = defaultdict(list, {
                name: [data['description']] for name, data in self.entities.items()
            })
            entity_types = {name: data['type'] for name, data in self.entities.items()}
            entity_sources = defaultdict(set, {
                name: set(data['source_ids']) for name, data in self.entities.items()
            })
        else:
            entity_descriptions = defaultdict(list)
            entity_types = {}
            entity_sources = defaultdict(set)
        
        # æ”¶é›†æ–°å®ä½“
        for chunk_id, chunk_data in enumerate(self.text_chunks):
            entities = chunk_data.get('entities', [])
            
            for entity in entities:
                name = entity['name']
                entity_descriptions[name].append(entity['description'])
                entity_types[name] = entity['type']
                entity_sources[name].add(chunk_id)
        
        logger.info(f"  å‘ç° {len(entity_descriptions)} ä¸ªå”¯ä¸€å®ä½“")
        
        # â˜…â˜…â˜… å®ä½“å¯¹é½ â˜…â˜…â˜…
        logger.info("  ğŸ”„ æ‰§è¡Œå®ä½“å¯¹é½...")
        self.entity_alignments = self.entity_aligner.align_entities(
            {name: {} for name in entity_descriptions.keys()}
        )
        
        # ç»Ÿè®¡å¯¹é½ç»“æœ
        aligned_groups = defaultdict(list)
        for original_name, alignment in self.entity_alignments.items():
            aligned_groups[alignment.canonical_name].append(original_name)
        
        merged_count = sum(1 for aliases in aligned_groups.values() if len(aliases) > 1)
        logger.info(f"  å¯¹é½å®Œæˆ: {len(entity_descriptions)} ä¸ªå®ä½“ â†’ {len(aligned_groups)} ä¸ªæ ‡å‡†å®ä½“ (åˆå¹¶äº† {merged_count} ç»„)")
        
        # ä½¿ç”¨å¯¹é½åçš„å®ä½“
        aligned_descriptions = defaultdict(list)
        aligned_types = {}
        aligned_sources = defaultdict(set)
        
        for original_name, alignment in self.entity_alignments.items():
            canonical = alignment.canonical_name
            aligned_descriptions[canonical].extend(entity_descriptions[original_name])
            aligned_types[canonical] = entity_types[original_name]
            aligned_sources[canonical].update(entity_sources[original_name])
        
        # å¼‚æ­¥ç”Ÿæˆå®ä½“æ‘˜è¦
        logger.info("  ç”Ÿæˆå®ä½“æ‘˜è¦...")
        tasks = []
        entity_names = []
        
        for entity_name, descriptions in aligned_descriptions.items():
            if len(descriptions) > 1 or not incremental or entity_name not in self.entities:
                tasks.append(self.summarize_entity(entity_name, descriptions))
                entity_names.append(entity_name)
        
        if tasks:
            summaries = await asyncio.gather(*tasks)
            
            for entity_name, summary in zip(entity_names, summaries):
                self.entities[entity_name] = {
                    'description': summary,
                    'type': aligned_types[entity_name],
                    'source_ids': list(aligned_sources[entity_name]),
                    'aliases': [alias for alias, align in self.entity_alignments.items() 
                               if align.canonical_name == entity_name and alias != entity_name]
                }
        
        # åˆå¹¶å…³ç³» (ä½¿ç”¨å¯¹é½åçš„å®ä½“å)
        relationship_map = defaultdict(lambda: {'descriptions': [], 'strengths': [], 'sources': set()})
        
        for chunk_id, chunk_data in enumerate(self.text_chunks):
            for rel in chunk_data.get('relationships', []):
                # è·å–å¯¹é½åçš„å®ä½“å
                source = self.entity_alignments.get(rel['source'], 
                        EntityAlignment(rel['source'], [rel['source']], 1.0)).canonical_name
                target = self.entity_alignments.get(rel['target'], 
                        EntityAlignment(rel['target'], [rel['target']], 1.0)).canonical_name
                
                key = (source, target)
                relationship_map[key]['descriptions'].append(rel['description'])
                relationship_map[key]['strengths'].append(rel.get('strength', 5))
                relationship_map[key]['sources'].add(chunk_id)
        
        self.relationships = []
        for (source, target), data in relationship_map.items():
            if source in self.entities and target in self.entities:
                self.relationships.append({
                    'source': source,
                    'target': target,
                    'description': '; '.join(data['descriptions']),
                    'weight': float(np.mean(data['strengths'])),
                    'source_ids': list(data['sources'])
                })
        
        logger.info(f"  âœ… å®Œæˆ: {len(self.entities)} å®ä½“, {len(self.relationships)} å…³ç³»")
        self.needs_rebuild['entities'] = False
    
    async def build_graph(self):
        """å¼‚æ­¥æ„å»ºçŸ¥è¯†å›¾è°±"""
        logger.info("ğŸ•¸ï¸ æ„å»ºçŸ¥è¯†å›¾è°±...")
        
        self.graph = nx.Graph()
        
        for entity_name, entity_data in self.entities.items():
            self.graph.add_node(
                entity_name,
                type=entity_data['type'],
                description=entity_data['description']
            )
        
        for rel in self.relationships:
            self.graph.add_edge(
                rel['source'],
                rel['target'],
                weight=rel['weight'],
                description=rel['description']
            )
        
        logger.info(f"  âœ… å›¾è°±: {self.graph.number_of_nodes()} èŠ‚ç‚¹, {self.graph.number_of_edges()} è¾¹")
        self.needs_rebuild['graph'] = False
    
    def detect_hierarchical_communities(self, max_level: int = 3):
        """å±‚æ¬¡åŒ–ç¤¾åŒºæ£€æµ‹ (ä¿æŒåŒæ­¥ï¼Œå› ä¸º community_louvain æ˜¯åŒæ­¥çš„)"""
        logger.info("ğŸ‘¥ ç¤¾åŒºæ£€æµ‹...")
        
        self.communities = {}
        current_graph = self.graph.copy()
        
        for level in range(max_level):
            partition = community_louvain.best_partition(
                current_graph,
                weight='weight',
                resolution=1.0
            )
            
            communities_at_level = defaultdict(list)
            for node, comm_id in partition.items():
                communities_at_level[comm_id].append(node)
            
            self.communities[level] = dict(communities_at_level)
            logger.info(f"  Level {level}: {len(communities_at_level)} ä¸ªç¤¾åŒº")
            
            if len(communities_at_level) <= 1:
                break
            
            # æ„å»ºä¸‹ä¸€å±‚
            next_graph = nx.Graph()
            for comm_id in communities_at_level.keys():
                next_graph.add_node(f"comm_{level}_{comm_id}")
            
            for u, v, data in current_graph.edges(data=True):
                comm_u = partition[u]
                comm_v = partition[v]
                if comm_u != comm_v:
                    edge_key = (f"comm_{level}_{comm_u}", f"comm_{level}_{comm_v}")
                    if next_graph.has_edge(*edge_key):
                        next_graph[edge_key[0]][edge_key[1]]['weight'] += data.get('weight', 1)
                    else:
                        next_graph.add_edge(*edge_key, weight=data.get('weight', 1))
            
            current_graph = next_graph
        
        self.needs_rebuild['communities'] = False
    
    async def generate_community_summary(self, level: int, community_id: int) -> str:
        """å¼‚æ­¥ç”Ÿæˆç¤¾åŒºæ‘˜è¦"""
        nodes = self.communities[level][community_id]
        
        entities_info = []
        for node in nodes[:20]:
            if node in self.entities:
                entities_info.append(
                    f"- {node} ({self.entities[node]['type']}): "
                    f"{self.entities[node]['description'][:200]}"
                )
        
        relationships_info = []
        for rel in self.relationships:
            if rel['source'] in nodes and rel['target'] in nodes:
                relationships_info.append(
                    f"- {rel['source']} â†’ {rel['target']}: {rel['description'][:150]}"
                )
        
        prompt = f"""ç”Ÿæˆç¤¾åŒºæ‘˜è¦ï¼ˆ300-400è¯ï¼‰ï¼š

å®ä½“:
{chr(10).join(entities_info)}

å…³ç³»:
{chr(10).join(relationships_info[:15])}

åŒ…æ‹¬ï¼šä¸»é¢˜ã€å…³é”®å®ä½“ã€å…³é”®å‘ç°ã€è¿æ¥æ€§ã€‚åªè¿”å›æ‘˜è¦ã€‚"""

        response_text = await self.llm_client.chat(
            messages=[{"role": "user", "content": prompt}],
            temperature=0.3,
            max_tokens=500
        )
        
        return response_text.strip()
    
    async def generate_all_community_summaries(self):
        """å¼‚æ­¥ç”Ÿæˆæ‰€æœ‰ç¤¾åŒºæ‘˜è¦"""
        logger.info("ğŸ“ ç”Ÿæˆç¤¾åŒºæ‘˜è¦...")
        
        self.community_summaries = {}
        tasks = []
        keys = []
        
        for level, communities in self.communities.items():
            for comm_id in communities.keys():
                tasks.append(self.generate_community_summary(level, comm_id))
                keys.append((level, comm_id))
        
        summaries = await asyncio.gather(*tasks)
        
        for key, summary in zip(keys, summaries):
            self.community_summaries[key] = summary
        
        logger.info(f"  âœ… ç”Ÿæˆäº† {len(self.community_summaries)} ä¸ªç¤¾åŒºæ‘˜è¦")
        self.needs_rebuild['summaries'] = False
    
    async def build_community_summary_index(self):
        """å¼‚æ­¥æ„å»ºå‘é‡ç´¢å¼•"""
        logger.info("ğŸ” æ„å»ºå‘é‡ç´¢å¼•...")
        
        summaries = []
        summary_metadata = []
        
        for (level, comm_id), summary in self.community_summaries.items():
            summaries.append(summary)
            summary_metadata.append({
                'level': level,
                'community_id': comm_id,
                'summary': summary
            })
        
        if not summaries:
            logger.warning("âš ï¸ æ²¡æœ‰ç¤¾åŒºæ‘˜è¦å¯ç´¢å¼•")
            return
        
        # æ‰¹é‡ç”Ÿæˆ embeddings
        batch_size = 100
        embeddings = []
        
        logger.info(f"  ç”Ÿæˆ {len(summaries)} ä¸ªæ‘˜è¦çš„å‘é‡...")
        for i in range(0, len(summaries), batch_size):
            batch = summaries[i:i + batch_size]
            batch_embeddings = await self.embedding_client.embed(batch)
            embeddings.extend(batch_embeddings)
        
        self.community_embeddings = summary_metadata
        
        # æ„å»º FAISS
        embeddings_array = np.array(embeddings, dtype='float32')
        self.community_summary_index = faiss.IndexFlatIP(self.dimension)
        faiss.normalize_L2(embeddings_array)
        self.community_summary_index.add(embeddings_array)
        
        logger.info(f"  âœ… ç´¢å¼•å®Œæˆ: {len(embeddings)} ä¸ªç¤¾åŒº")
        self.needs_rebuild['index'] = False
    
    # ==================== ç´¢å¼•æ„å»º (æ™ºèƒ½å¢é‡) ====================
    
    async def rebuild_index(self, force_full: bool = False):
        """
        æ™ºèƒ½é‡å»ºç´¢å¼•
        
        Args:
            force_full: å¼ºåˆ¶å…¨é‡é‡å»º
        """
        logger.info("=" * 60)
        logger.info(f"ğŸ”„ {'å…¨é‡' if force_full else 'å¢é‡'}é‡å»º GraphRAG ç´¢å¼•")
        logger.info("=" * 60)
        
        if not self.text_chunks:
            logger.error("âŒ text_chunks ä¸ºç©ºï¼è¯·å…ˆæ·»åŠ æ–‡æ¡£ã€‚")
            raise RuntimeError("æ²¡æœ‰æ–‡æ¡£å¯ä»¥ç´¢å¼•ï¼Œè¯·å…ˆä¸Šä¼ æ–‡æ¡£")
        
        logger.info(f"ğŸ“Š æ•°æ®ç»Ÿè®¡: {len(self.text_chunks)} chunks, {len(self.documents)} æ–‡æ¡£")
        
        # æ ¹æ®æ ‡è®°å†³å®šæ‰§è¡Œå“ªäº›é˜¶æ®µ
        if force_full or self.needs_rebuild['entities']:
            logger.info("[1/5] åˆå¹¶å®ä½“å’Œå…³ç³»...")
            await self.merge_entities_and_relationships(incremental=not force_full)
            logger.info(f"  å®Œæˆ: {len(self.entities)} å®ä½“, {len(self.relationships)} å…³ç³»")
        else:
            logger.info("[1/5] è·³è¿‡ (å®ä½“å·²æ˜¯æœ€æ–°)")
        
        if not self.entities:
            logger.error("âŒ æ²¡æœ‰æå–åˆ°å®ä½“ï¼è¯·æ£€æŸ¥æ–‡æ¡£å†…å®¹æˆ– LLM é…ç½®")
            raise RuntimeError("æœªèƒ½æå–å®ä½“ï¼Œç´¢å¼•æ„å»ºå¤±è´¥")
        
        if force_full or self.needs_rebuild['graph']:
            logger.info("[2/5] æ„å»ºçŸ¥è¯†å›¾è°±...")
            await self.build_graph()
            logger.info(f"  å®Œæˆ: {self.graph.number_of_nodes()} èŠ‚ç‚¹, {self.graph.number_of_edges()} è¾¹")
        else:
            logger.info("[2/5] è·³è¿‡ (å›¾è°±å·²æ˜¯æœ€æ–°)")
        
        if force_full or self.needs_rebuild['communities']:
            logger.info("[3/5] ç¤¾åŒºæ£€æµ‹...")
            loop = asyncio.get_event_loop()
            await loop.run_in_executor(None, self.detect_hierarchical_communities)
        else:
            logger.info("[3/5] è·³è¿‡ (ç¤¾åŒºå·²æ˜¯æœ€æ–°)")
        
        if force_full or self.needs_rebuild['summaries']:
            logger.info("[4/5] ç”Ÿæˆç¤¾åŒºæ‘˜è¦...")
            await self.generate_all_community_summaries()
        else:
            logger.info("[4/5] è·³è¿‡ (æ‘˜è¦å·²æ˜¯æœ€æ–°)")
        
        if force_full or self.needs_rebuild['index']:
            logger.info("[5/5] æ„å»ºå‘é‡ç´¢å¼•...")
            await self.build_community_summary_index()
        else:
            logger.info("[5/5] è·³è¿‡ (ç´¢å¼•å·²æ˜¯æœ€æ–°)")
        
        # è‡ªåŠ¨ä¿å­˜
        await self.save("default")
        logger.info("ğŸ’¾ ç´¢å¼•é‡å»ºåå·²è‡ªåŠ¨ä¿å­˜")
        
        logger.info("=" * 60)
        logger.info("âœ… ç´¢å¼•é‡å»ºå®Œæˆ!")
        logger.info("=" * 60)
    
    # ==================== æŸ¥è¯¢ (å¼‚æ­¥) ====================
    
    async def global_query(self, question: str, top_k_communities: int = 10, 
                          simple_mode: bool = False) -> str:
        """å¼‚æ­¥æŸ¥è¯¢çŸ¥è¯†åº“"""
        if self.community_summary_index is None:
            raise RuntimeError("ç´¢å¼•æœªæ„å»ºï¼Œè¯·å…ˆä¸Šä¼ æ–‡æ¡£å¹¶é‡å»ºç´¢å¼•")
        
        # æ£€ç´¢ç¤¾åŒº
        query_embeddings = await self.embedding_client.embed([question])
        query_embedding = np.array(query_embeddings, dtype='float32')
        faiss.normalize_L2(query_embedding)
        
        scores, indices = self.community_summary_index.search(
            query_embedding, 
            min(top_k_communities, len(self.community_embeddings))
        )
        
        # ç®€å•æ¨¡å¼ï¼šç›´æ¥è¿”å›ç¤¾åŒºæ‘˜è¦
        if simple_mode:
            search_results = []
            threshold = 0.5
            
            for idx, score in zip(indices[0], scores[0]):
                if score >= threshold:
                    search_results.append(self.community_embeddings[idx]['summary'])
            
            if not search_results:
                return "æŠ±æ­‰ï¼Œæœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚"
            
            return "\n\n".join([f"ç¤¾åŒºæ‘˜è¦ {i+1}\n{res}" 
                              for i, res in enumerate(search_results)])
        
        # Map-Reduce æ¨¡å¼
        tasks = []
        valid_indices = []
        
        for idx, score in zip(indices[0], scores[0]):
            if idx < len(self.community_embeddings):
                comm_data = self.community_embeddings[idx]
                tasks.append(self._ask_community(question, comm_data['summary']))
                valid_indices.append((idx, score, comm_data))
        
        answers = await asyncio.gather(*tasks)
        
        community_answers = []
        for (idx, score, comm_data), answer in zip(valid_indices, answers):
            if answer and len(answer.strip()) > 10:
                community_answers.append({
                    'level': comm_data['level'],
                    'community_id': comm_data['community_id'],
                    'content': answer,
                    'score': float(score)
                })
        
        return await self._reduce_answers(question, community_answers)
    
    async def _ask_community(self, question: str, community_summary: str) -> str:
        """å¼‚æ­¥è¯¢é—®å•ä¸ªç¤¾åŒº"""
        prompt = f"""åŸºäºç¤¾åŒºä¿¡æ¯å›ç­”é—®é¢˜ï¼ˆ2-3å¥è¯ï¼‰ã€‚å¦‚æœæ— å…³ï¼Œå›ç­”"æ— ç›¸å…³ä¿¡æ¯"ã€‚

ç¤¾åŒºä¿¡æ¯:
{community_summary}

é—®é¢˜: {question}

åªè¿”å›ç­”æ¡ˆã€‚"""
        
        try:
            response_text = await self.llm_client.chat(
                messages=[{"role": "user", "content": prompt}],
                temperature=0.5,
                max_tokens=150
            )
            return response_text.strip()
        except Exception as e:
            logger.error(f"ç¤¾åŒºæŸ¥è¯¢å¤±è´¥: {e}")
            return ""
    
    async def _reduce_answers(self, question: str, community_answers: List[Dict]) -> str:
        """å¼‚æ­¥ç»¼åˆç­”æ¡ˆ"""
        if not community_answers:
            return "æŠ±æ­‰ï¼Œåœ¨çŸ¥è¯†å›¾è°±ä¸­æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚"
        
        community_answers.sort(key=lambda x: x['score'], reverse=True)
        
        answers_text = []
        for i, ans_data in enumerate(community_answers[:10], 1):
            if ans_data['content'].lower() != "æ— ç›¸å…³ä¿¡æ¯":
                answers_text.append(f"{i}. {ans_data['content']}")
        
        if not answers_text:
            return "æŠ±æ­‰ï¼Œæ‰¾åˆ°çš„ä¿¡æ¯ä¸é—®é¢˜ä¸å¤ªç›¸å…³ã€‚"
        
        combined = "\n".join(answers_text)
        
        prompt = f"""ç»¼åˆä»¥ä¸‹ç­”æ¡ˆä¸ºä¸€ä¸ªè¿è´¯çš„æœ€ç»ˆç­”æ¡ˆï¼ˆ200-400è¯ï¼‰ï¼š

é—®é¢˜: {question}

å„ç¤¾åŒºç­”æ¡ˆ:
{combined}

è¦æ±‚: æ•´åˆä¿¡æ¯ã€æ¶ˆé™¤å†—ä½™ã€ä¿æŒæ¸…æ™°ã€å‘ˆç°ä¸åŒè§‚ç‚¹ï¼ˆå¦‚æœ‰ï¼‰ã€‚

åªè¿”å›æœ€ç»ˆç­”æ¡ˆã€‚"""

        response_text = await self.llm_client.chat(
            messages=[{"role": "user", "content": prompt}],
            temperature=0.3,
            max_tokens=600
        )
        
        return response_text.strip()
    
    # ==================== æŒä¹…åŒ– (å¼‚æ­¥) ====================
    
    async def save(self, name: str = "default"):
        """å¼‚æ­¥ä¿å­˜çŸ¥è¯†åº“"""
        save_dir = self.storage_dir / name
        save_dir.mkdir(parents=True, exist_ok=True)
        
        logger.info(f"ğŸ’¾ ä¿å­˜çŸ¥è¯†åº“: {save_dir}")
        
        loop = asyncio.get_event_loop()
        
        # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œ IO æ“ä½œ
        await loop.run_in_executor(None, self._save_sync, save_dir)
        
        logger.info(f"  âœ… ä¿å­˜å®Œæˆ: {len(self.documents)} æ–‡æ¡£, {len(self.text_chunks)} chunks")
    
    def _save_sync(self, save_dir: Path):
        """åŒæ­¥ä¿å­˜é€»è¾‘"""
        # ä¿å­˜æ–‡æ¡£
        with open(save_dir / "documents.json", 'w', encoding='utf-8') as f:
            json.dump(self.documents, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜ UUID æ˜ å°„
        with open(save_dir / "uuid_mappings.json", 'w', encoding='utf-8') as f:
            json.dump({
                'uuid_to_docid': self.uuid_to_docid,
                'docid_to_uuid': self.docid_to_uuid
            }, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜å›¾æ•°æ®
        with open(save_dir / "graph_data.pkl", 'wb') as f:
            pickle.dump({
                'text_chunks': self.text_chunks,
                'chunk_to_doc': self.chunk_to_doc,
                'entities': self.entities,
                'entity_alignments': {k: (v.canonical_name, v.aliases, v.similarity) 
                                     for k, v in self.entity_alignments.items()},
                'relationships': self.relationships,
                'claims': self.claims,
                'communities': self.communities,
                'community_summaries': self.community_summaries,
                'community_embeddings': self.community_embeddings,
                'needs_rebuild': self.needs_rebuild,
            }, f)
        
        # ä¿å­˜å›¾
        with open(save_dir / "graph.gpickle", 'wb') as f:
            pickle.dump(self.graph, f)
        
        # ä¿å­˜ FAISS
        if self.community_summary_index:
            faiss.write_index(self.community_summary_index, 
                            str(save_dir / "faiss_index.bin"))
    
    async def load(self, name: str = "default"):
        """å¼‚æ­¥åŠ è½½çŸ¥è¯†åº“"""
        load_dir = self.storage_dir / name
        
        if not load_dir.exists():
            raise FileNotFoundError(f"çŸ¥è¯†åº“ä¸å­˜åœ¨: {load_dir}")
        
        logger.info(f"ğŸ“‚ åŠ è½½çŸ¥è¯†åº“: {load_dir}")
        
        loop = asyncio.get_event_loop()
        await loop.run_in_executor(None, self._load_sync, load_dir)
        
        logger.info(f"  âœ… åŠ è½½å®Œæˆ: {len(self.documents)} æ–‡æ¡£, "
                   f"{len(self.text_chunks)} chunks, {len(self.entities)} å®ä½“")
    
    def _load_sync(self, load_dir: Path):
        """åŒæ­¥åŠ è½½é€»è¾‘"""
        # åŠ è½½æ–‡æ¡£
        with open(load_dir / "documents.json", 'r', encoding='utf-8') as f:
            self.documents = json.load(f)
        
        # åŠ è½½ UUID æ˜ å°„
        uuid_path = load_dir / "uuid_mappings.json"
        if uuid_path.exists():
            with open(uuid_path, 'r', encoding='utf-8') as f:
                mappings = json.load(f)
                self.uuid_to_docid = mappings['uuid_to_docid']
                self.docid_to_uuid = mappings['docid_to_uuid']
        
        # åŠ è½½å›¾æ•°æ®
        with open(load_dir / "graph_data.pkl", 'rb') as f:
            data = pickle.load(f)
            self.text_chunks = data['text_chunks']
            self.chunk_to_doc = data['chunk_to_doc']
            self.entities = data['entities']
            
            # æ¢å¤å®ä½“å¯¹é½
            if 'entity_alignments' in data:
                self.entity_alignments = {
                    k: EntityAlignment(v[0], v[1], v[2]) 
                    for k, v in data['entity_alignments'].items()
                }
            
            self.relationships = data['relationships']
            self.claims = data['claims']
            self.communities = data['communities']
            self.community_summaries = data['community_summaries']
            self.community_embeddings = data['community_embeddings']
            
            if 'needs_rebuild' in data:
                self.needs_rebuild = data['needs_rebuild']
        
        # åŠ è½½å›¾
        with open(load_dir / "graph.gpickle", 'rb') as f:
            self.graph = pickle.load(f)
        
        # åŠ è½½ FAISS
        index_path = load_dir / "faiss_index.bin"
        if index_path.exists():
            self.community_summary_index = faiss.read_index(str(index_path))
    
    async def close(self):
        """å…³é—­å®¢æˆ·ç«¯è¿æ¥"""
        await self.llm_client.close()
        await self.embedding_client.close()


